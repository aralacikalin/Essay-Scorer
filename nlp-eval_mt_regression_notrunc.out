comet_ml is installed but `COMET_API_KEY` is not set.
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertModel: ['distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'vocab_projector.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'vocab_transform.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.layer.8.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.weight', 'pooler.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.weight', 'pooler.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.4.attention.output.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.3.attention.output.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00,  2.75ba/s]100%|██████████| 1/1 [00:00<00:00,  2.74ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00,  2.92ba/s]100%|██████████| 1/1 [00:00<00:00,  2.92ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00,  8.25ba/s]100%|██████████| 1/1 [00:00<00:00,  8.23ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00,  8.03ba/s]100%|██████████| 1/1 [00:00<00:00,  7.30ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00,  6.62ba/s]100%|██████████| 1/1 [00:00<00:00,  6.61ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00,  5.80ba/s]100%|██████████| 1/1 [00:00<00:00,  5.79ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00,  6.29ba/s]100%|██████████| 1/1 [00:00<00:00,  6.28ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00,  5.04ba/s]100%|██████████| 1/1 [00:00<00:00,  5.03ba/s]
The following columns in the evaluation set  don't have a corresponding argument in `FakeNewsClassifierModel.forward` and have been ignored: rater2_domain1, rater1_trait3, rater2_trait5, rater2_trait6, essay, rater3_trait2, Unnamed: 0, rater3_trait4, rater2_trait2, rater3_trait6, domain2_score, rater3_trait5, rater2_trait3, __index_level_0__, rater1_trait2, rater3_domain1, essay_set, token_type_ids, rater3_trait3, rater1_trait6, rater1_domain2, rater2_domain2, rater3_trait1, essay_id, rater1_trait5, rater2_trait4, rater1_trait4, rater1_trait1, rater2_trait1, rater1_domain1. If rater2_domain1, rater1_trait3, rater2_trait5, rater2_trait6, essay, rater3_trait2, Unnamed: 0, rater3_trait4, rater2_trait2, rater3_trait6, domain2_score, rater3_trait5, rater2_trait3, __index_level_0__, rater1_trait2, rater3_domain1, essay_set, token_type_ids, rater3_trait3, rater1_trait6, rater1_domain2, rater2_domain2, rater3_trait1, essay_id, rater1_trait5, rater2_trait4, rater1_trait4, rater1_trait1, rater2_trait1, rater1_domain1 are not expected by `FakeNewsClassifierModel.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 357
  Batch size = 4
  0%|          | 0/90 [00:00<?, ?it/s]  7%|▋         | 6/90 [00:00<00:01, 57.36it/s] 13%|█▎        | 12/90 [00:00<00:01, 53.20it/s] 21%|██        | 19/90 [00:00<00:01, 59.98it/s] 29%|██▉       | 26/90 [00:00<00:01, 63.30it/s] 37%|███▋      | 33/90 [00:00<00:00, 65.23it/s] 44%|████▍     | 40/90 [00:00<00:00, 66.45it/s] 52%|█████▏    | 47/90 [00:00<00:00, 67.10it/s] 60%|██████    | 54/90 [00:00<00:00, 67.60it/s] 68%|██████▊   | 61/90 [00:00<00:00, 67.73it/s] 76%|███████▌  | 68/90 [00:01<00:00, 67.98it/s] 83%|████████▎ | 75/90 [00:01<00:00, 68.15it/s] 91%|█████████ | 82/90 [00:01<00:00, 68.30it/s] 99%|█████████▉| 89/90 [00:01<00:00, 68.39it/s]100%|██████████| 90/90 [00:01<00:00, 66.22it/s]
The following columns in the evaluation set  don't have a corresponding argument in `FakeNewsClassifierModel.forward` and have been ignored: rater2_domain1, rater1_trait3, rater2_trait5, rater2_trait6, essay, rater3_trait2, Unnamed: 0, rater3_trait4, rater2_trait2, rater3_trait6, domain2_score, rater3_trait5, rater2_trait3, __index_level_0__, rater1_trait2, rater3_domain1, essay_set, token_type_ids, rater3_trait3, rater1_trait6, rater1_domain2, rater2_domain2, rater3_trait1, essay_id, rater1_trait5, rater2_trait4, rater1_trait4, rater1_trait1, rater2_trait1, rater1_domain1. If rater2_domain1, rater1_trait3, rater2_trait5, rater2_trait6, essay, rater3_trait2, Unnamed: 0, rater3_trait4, rater2_trait2, rater3_trait6, domain2_score, rater3_trait5, rater2_trait3, __index_level_0__, rater1_trait2, rater3_domain1, essay_set, token_type_ids, rater3_trait3, rater1_trait6, rater1_domain2, rater2_domain2, rater3_trait1, essay_id, rater1_trait5, rater2_trait4, rater1_trait4, rater1_trait1, rater2_trait1, rater1_domain1 are not expected by `FakeNewsClassifierModel.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 360
  Batch size = 4
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
{'eval_kappa': 0.5389936816861136, 'eval_loss': 1.7676773071289062, 'eval_runtime': 2.0641, 'eval_samples_per_second': 172.955, 'eval_steps_per_second': 43.602}
----------------------------------------------------------
  0%|          | 0/90 [00:00<?, ?it/s]  9%|▉         | 8/90 [00:00<00:01, 78.65it/s] 18%|█▊        | 16/90 [00:00<00:01, 72.36it/s] 27%|██▋       | 24/90 [00:00<00:00, 70.66it/s] 36%|███▌      | 32/90 [00:00<00:00, 69.90it/s] 44%|████▍     | 40/90 [00:00<00:00, 69.37it/s] 52%|█████▏    | 47/90 [00:00<00:00, 69.19it/s] 60%|██████    | 54/90 [00:00<00:00, 68.92it/s] 68%|██████▊   | 61/90 [00:00<00:00, 68.70it/s] 76%|███████▌  | 68/90 [00:00<00:00, 68.61it/s] 83%|████████▎ | 75/90 [00:01<00:00, 68.58it/s] 91%|█████████ | 82/90 [00:01<00:00, 68.56it/s] 99%|█████████▉| 89/90 [00:01<00:00, 68.51it/s]100%|██████████| 90/90 [00:01<00:00, 69.14it/s]
The following columns in the evaluation set  don't have a corresponding argument in `FakeNewsClassifierModel.forward` and have been ignored: rater2_domain1, rater1_trait3, rater2_trait5, rater2_trait6, essay, rater3_trait2, Unnamed: 0, rater3_trait4, rater2_trait2, rater3_trait6, domain2_score, rater3_trait5, rater2_trait3, __index_level_0__, rater1_trait2, rater3_domain1, essay_set, token_type_ids, rater3_trait3, rater1_trait6, rater1_domain2, rater2_domain2, rater3_trait1, essay_id, rater1_trait5, rater2_trait4, rater1_trait4, rater1_trait1, rater2_trait1, rater1_domain1. If rater2_domain1, rater1_trait3, rater2_trait5, rater2_trait6, essay, rater3_trait2, Unnamed: 0, rater3_trait4, rater2_trait2, rater3_trait6, domain2_score, rater3_trait5, rater2_trait3, __index_level_0__, rater1_trait2, rater3_domain1, essay_set, token_type_ids, rater3_trait3, rater1_trait6, rater1_domain2, rater2_domain2, rater3_trait1, essay_id, rater1_trait5, rater2_trait4, rater1_trait4, rater1_trait1, rater2_trait1, rater1_domain1 are not expected by `FakeNewsClassifierModel.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 345
  Batch size = 4
[0, 1, 2, 3, 4, 5, 6]
{'eval_kappa': 0.5904278659358719, 'eval_loss': 0.3543785810470581, 'eval_runtime': 1.3156, 'eval_samples_per_second': 273.636, 'eval_steps_per_second': 68.409}
----------------------------------------------------------
  0%|          | 0/87 [00:00<?, ?it/s] 13%|█▎        | 11/87 [00:00<00:00, 100.34it/s] 25%|██▌       | 22/87 [00:00<00:00, 95.21it/s]  37%|███▋      | 32/87 [00:00<00:00, 93.67it/s] 48%|████▊     | 42/87 [00:00<00:00, 92.95it/s] 60%|█████▉    | 52/87 [00:00<00:00, 92.58it/s] 71%|███████▏  | 62/87 [00:00<00:00, 92.30it/s] 83%|████████▎ | 72/87 [00:00<00:00, 92.17it/s] 94%|█████████▍| 82/87 [00:00<00:00, 92.10it/s]100%|██████████| 87/87 [00:00<00:00, 92.67it/s]
The following columns in the evaluation set  don't have a corresponding argument in `FakeNewsClassifierModel.forward` and have been ignored: rater2_domain1, rater1_trait3, rater2_trait5, rater2_trait6, essay, rater3_trait2, Unnamed: 0, rater3_trait4, rater2_trait2, rater3_trait6, domain2_score, rater3_trait5, rater2_trait3, __index_level_0__, rater1_trait2, rater3_domain1, essay_set, token_type_ids, rater3_trait3, rater1_trait6, rater1_domain2, rater2_domain2, rater3_trait1, essay_id, rater1_trait5, rater2_trait4, rater1_trait4, rater1_trait1, rater2_trait1, rater1_domain1. If rater2_domain1, rater1_trait3, rater2_trait5, rater2_trait6, essay, rater3_trait2, Unnamed: 0, rater3_trait4, rater2_trait2, rater3_trait6, domain2_score, rater3_trait5, rater2_trait3, __index_level_0__, rater1_trait2, rater3_domain1, essay_set, token_type_ids, rater3_trait3, rater1_trait6, rater1_domain2, rater2_domain2, rater3_trait1, essay_id, rater1_trait5, rater2_trait4, rater1_trait4, rater1_trait1, rater2_trait1, rater1_domain1 are not expected by `FakeNewsClassifierModel.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 354
  Batch size = 4
[0, 1, 2, 3]
{'eval_kappa': 0.5466710721956056, 'eval_loss': 0.5275678038597107, 'eval_runtime': 0.9484, 'eval_samples_per_second': 363.777, 'eval_steps_per_second': 91.735}
----------------------------------------------------------
  0%|          | 0/89 [00:00<?, ?it/s] 10%|█         | 9/89 [00:00<00:00, 80.11it/s] 20%|██        | 18/89 [00:00<00:00, 74.42it/s] 29%|██▉       | 26/89 [00:00<00:00, 72.86it/s] 38%|███▊      | 34/89 [00:00<00:00, 72.04it/s] 47%|████▋     | 42/89 [00:00<00:00, 71.58it/s] 56%|█████▌    | 50/89 [00:00<00:00, 71.42it/s] 65%|██████▌   | 58/89 [00:00<00:00, 71.18it/s] 74%|███████▍  | 66/89 [00:00<00:00, 70.98it/s] 83%|████████▎ | 74/89 [00:01<00:00, 70.90it/s] 92%|█████████▏| 82/89 [00:01<00:00, 70.85it/s]100%|██████████| 89/89 [00:01<00:00, 71.76it/s]
The following columns in the evaluation set  don't have a corresponding argument in `FakeNewsClassifierModel.forward` and have been ignored: rater2_domain1, rater1_trait3, rater2_trait5, rater2_trait6, essay, rater3_trait2, Unnamed: 0, rater3_trait4, rater2_trait2, rater3_trait6, domain2_score, rater3_trait5, rater2_trait3, __index_level_0__, rater1_trait2, rater3_domain1, essay_set, token_type_ids, rater3_trait3, rater1_trait6, rater1_domain2, rater2_domain2, rater3_trait1, essay_id, rater1_trait5, rater2_trait4, rater1_trait4, rater1_trait1, rater2_trait1, rater1_domain1. If rater2_domain1, rater1_trait3, rater2_trait5, rater2_trait6, essay, rater3_trait2, Unnamed: 0, rater3_trait4, rater2_trait2, rater3_trait6, domain2_score, rater3_trait5, rater2_trait3, __index_level_0__, rater1_trait2, rater3_domain1, essay_set, token_type_ids, rater3_trait3, rater1_trait6, rater1_domain2, rater2_domain2, rater3_trait1, essay_id, rater1_trait5, rater2_trait4, rater1_trait4, rater1_trait1, rater2_trait1, rater1_domain1 are not expected by `FakeNewsClassifierModel.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 361
  Batch size = 4
[0, 1, 2, 3]
{'eval_kappa': 0.6594374172099426, 'eval_loss': 0.3932296931743622, 'eval_runtime': 1.2534, 'eval_samples_per_second': 282.421, 'eval_steps_per_second': 71.004}
----------------------------------------------------------
  0%|          | 0/91 [00:00<?, ?it/s]  9%|▉         | 8/91 [00:00<00:01, 79.05it/s] 18%|█▊        | 16/91 [00:00<00:01, 72.73it/s] 26%|██▋       | 24/91 [00:00<00:00, 70.95it/s] 35%|███▌      | 32/91 [00:00<00:00, 69.94it/s] 44%|████▍     | 40/91 [00:00<00:00, 69.41it/s] 52%|█████▏    | 47/91 [00:00<00:00, 69.13it/s] 59%|█████▉    | 54/91 [00:00<00:00, 68.94it/s] 67%|██████▋   | 61/91 [00:00<00:00, 68.84it/s] 75%|███████▍  | 68/91 [00:00<00:00, 68.77it/s] 82%|████████▏ | 75/91 [00:01<00:00, 68.73it/s] 90%|█████████ | 82/91 [00:01<00:00, 68.68it/s] 98%|█████████▊| 89/91 [00:01<00:00, 68.71it/s]100%|██████████| 91/91 [00:01<00:00, 69.60it/s]
The following columns in the evaluation set  don't have a corresponding argument in `FakeNewsClassifierModel.forward` and have been ignored: rater2_domain1, rater1_trait3, rater2_trait5, rater2_trait6, essay, rater3_trait2, Unnamed: 0, rater3_trait4, rater2_trait2, rater3_trait6, domain2_score, rater3_trait5, rater2_trait3, __index_level_0__, rater1_trait2, rater3_domain1, essay_set, token_type_ids, rater3_trait3, rater1_trait6, rater1_domain2, rater2_domain2, rater3_trait1, essay_id, rater1_trait5, rater2_trait4, rater1_trait4, rater1_trait1, rater2_trait1, rater1_domain1. If rater2_domain1, rater1_trait3, rater2_trait5, rater2_trait6, essay, rater3_trait2, Unnamed: 0, rater3_trait4, rater2_trait2, rater3_trait6, domain2_score, rater3_trait5, rater2_trait3, __index_level_0__, rater1_trait2, rater3_domain1, essay_set, token_type_ids, rater3_trait3, rater1_trait6, rater1_domain2, rater2_domain2, rater3_trait1, essay_id, rater1_trait5, rater2_trait4, rater1_trait4, rater1_trait1, rater2_trait1, rater1_domain1 are not expected by `FakeNewsClassifierModel.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 360
  Batch size = 4
[0, 1, 2, 3, 4]
{'eval_kappa': 0.7606134696599245, 'eval_loss': 1.5752555131912231, 'eval_runtime': 1.321, 'eval_samples_per_second': 273.274, 'eval_steps_per_second': 68.886}
----------------------------------------------------------
  0%|          | 0/90 [00:00<?, ?it/s]  9%|▉         | 8/90 [00:00<00:01, 78.95it/s] 18%|█▊        | 16/90 [00:00<00:01, 72.47it/s] 27%|██▋       | 24/90 [00:00<00:00, 70.53it/s] 36%|███▌      | 32/90 [00:00<00:00, 69.42it/s] 43%|████▎     | 39/90 [00:00<00:00, 68.24it/s] 51%|█████     | 46/90 [00:00<00:00, 66.46it/s] 59%|█████▉    | 53/90 [00:00<00:00, 65.25it/s] 67%|██████▋   | 60/90 [00:00<00:00, 64.27it/s] 74%|███████▍  | 67/90 [00:01<00:00, 64.01it/s] 82%|████████▏ | 74/90 [00:01<00:00, 63.67it/s] 90%|█████████ | 81/90 [00:01<00:00, 63.41it/s] 98%|█████████▊| 88/90 [00:01<00:00, 63.26it/s]100%|██████████| 90/90 [00:02<00:00, 34.83it/s]
The following columns in the evaluation set  don't have a corresponding argument in `FakeNewsClassifierModel.forward` and have been ignored: rater2_domain1, rater1_trait3, rater2_trait5, rater2_trait6, essay, rater3_trait2, Unnamed: 0, rater3_trait4, rater2_trait2, rater3_trait6, domain2_score, rater3_trait5, rater2_trait3, __index_level_0__, rater1_trait2, rater3_domain1, essay_set, token_type_ids, rater3_trait3, rater1_trait6, rater1_domain2, rater2_domain2, rater3_trait1, essay_id, rater1_trait5, rater2_trait4, rater1_trait4, rater1_trait1, rater2_trait1, rater1_domain1. If rater2_domain1, rater1_trait3, rater2_trait5, rater2_trait6, essay, rater3_trait2, Unnamed: 0, rater3_trait4, rater2_trait2, rater3_trait6, domain2_score, rater3_trait5, rater2_trait3, __index_level_0__, rater1_trait2, rater3_domain1, essay_set, token_type_ids, rater3_trait3, rater1_trait6, rater1_domain2, rater2_domain2, rater3_trait1, essay_id, rater1_trait5, rater2_trait4, rater1_trait4, rater1_trait1, rater2_trait1, rater1_domain1 are not expected by `FakeNewsClassifierModel.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 314
  Batch size = 4
[0, 1, 2, 3, 4]
{'eval_kappa': 0.7146157334542642, 'eval_loss': 0.33251190185546875, 'eval_runtime': 1.388, 'eval_samples_per_second': 259.372, 'eval_steps_per_second': 64.843}
----------------------------------------------------------
  0%|          | 0/79 [00:00<?, ?it/s]  8%|▊         | 6/79 [00:00<00:01, 58.32it/s] 16%|█▋        | 13/79 [00:00<00:01, 64.71it/s] 25%|██▌       | 20/79 [00:00<00:00, 66.58it/s] 34%|███▍      | 27/79 [00:00<00:00, 67.39it/s] 43%|████▎     | 34/79 [00:00<00:00, 67.85it/s] 52%|█████▏    | 41/79 [00:00<00:00, 68.13it/s] 61%|██████    | 48/79 [00:00<00:00, 68.27it/s] 70%|██████▉   | 55/79 [00:00<00:00, 68.38it/s] 78%|███████▊  | 62/79 [00:00<00:00, 68.51it/s] 87%|████████▋ | 69/79 [00:01<00:00, 68.54it/s] 96%|█████████▌| 76/79 [00:01<00:00, 68.54it/s]100%|██████████| 79/79 [00:01<00:00, 67.90it/s]
The following columns in the evaluation set  don't have a corresponding argument in `FakeNewsClassifierModel.forward` and have been ignored: rater2_domain1, rater1_trait3, rater2_trait5, rater2_trait6, essay, rater3_trait2, Unnamed: 0, rater3_trait4, rater2_trait2, rater3_trait6, domain2_score, rater3_trait5, rater2_trait3, __index_level_0__, rater1_trait2, rater3_domain1, essay_set, token_type_ids, rater3_trait3, rater1_trait6, rater1_domain2, rater2_domain2, rater3_trait1, essay_id, rater1_trait5, rater2_trait4, rater1_trait4, rater1_trait1, rater2_trait1, rater1_domain1. If rater2_domain1, rater1_trait3, rater2_trait5, rater2_trait6, essay, rater3_trait2, Unnamed: 0, rater3_trait4, rater2_trait2, rater3_trait6, domain2_score, rater3_trait5, rater2_trait3, __index_level_0__, rater1_trait2, rater3_domain1, essay_set, token_type_ids, rater3_trait3, rater1_trait6, rater1_domain2, rater2_domain2, rater3_trait1, essay_id, rater1_trait5, rater2_trait4, rater1_trait4, rater1_trait1, rater2_trait1, rater1_domain1 are not expected by `FakeNewsClassifierModel.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 145
  Batch size = 4
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
{'eval_kappa': 0.7710232655279485, 'eval_loss': 8.673046112060547, 'eval_runtime': 1.1875, 'eval_samples_per_second': 264.414, 'eval_steps_per_second': 66.525}
----------------------------------------------------------
  0%|          | 0/37 [00:00<?, ?it/s] 22%|██▏       | 8/37 [00:00<00:00, 78.73it/s] 43%|████▎     | 16/37 [00:00<00:00, 72.36it/s] 65%|██████▍   | 24/37 [00:00<00:00, 69.34it/s] 84%|████████▍ | 31/37 [00:00<00:00, 69.05it/s]100%|██████████| 37/37 [00:00<00:00, 70.04it/s]
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60]
{'eval_kappa': 0.2871856615086239, 'eval_loss': 80.5699691772461, 'eval_runtime': 0.5411, 'eval_samples_per_second': 267.984, 'eval_steps_per_second': 68.382}
----------------------------------------------------------
Average eval_Kappa:  0.6086210208972869
